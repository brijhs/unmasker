{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/brijhoward-\n",
      "[nltk_data]     sarin/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/brijhoward-\n",
      "[nltk_data]     sarin/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-11 20:45:34,578\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "#Necessary ml and ds package imports \n",
    "import torch\n",
    "import torch.nn.functional as F #for the activation function \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#Importing wordNet for Vocabulary training \n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "import random\n",
    "\n",
    "random.seed(7) # for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232k/232k [00:00<00:00, 1.23MB/s]\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer for definitions\n",
    "from torchtext.transforms import BERTTokenizer\n",
    "VOCAB_FILE = \"https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt\"\n",
    "tokenizer = BERTTokenizer(vocab_path=VOCAB_FILE, do_lower_case=True, return_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word masking function from string str\n",
    "def maskWord(string): \n",
    "\n",
    "    #Set type to string (wordnet includes integers)\n",
    "    if type(string) != str: \n",
    "        string = str(string)\n",
    "\n",
    "    str_len = len(string)\n",
    "    if str_len == 0: \n",
    "        return \n",
    "    ##random.setstate() <- helpful for reproducability later on \n",
    "    if str_len < 3: \n",
    "        choices = random.choices(range(str_len), k = 1)\n",
    "    else: \n",
    "        choices = random.choices(range(str_len), k = random.randint(2,str_len - 1))\n",
    "\n",
    "    ##Unpack string into a list of characters\n",
    "    str_list = [*string]\n",
    "    for index in choices: \n",
    "        str_list[index] = '_'\n",
    "    newstr = ' '.join(str_list)\n",
    "    return newstr\n",
    "\n",
    "#maskWord('example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions with wordNet to get definitions\n",
    "\n",
    "def getAllDefs(word): \n",
    "    defList = list()\n",
    "    for i in range(len(wordnet.synsets(word))): \n",
    "        defList.append(wordnet.synsets(word)[i].definition())\n",
    "    return defList\n",
    "\n",
    "def getFirstDef(word): \n",
    "    return wordnet.synsets(word)[0].definition()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "def getVocab(wordList): \n",
    "    vocab = [word for word in wordList]\n",
    "    print(vocab)\n",
    "    all_definitions = list() \n",
    "    for word in wordList: \n",
    "        all_definitions.append(getAllDefs(word))\n",
    "    \n",
    "    for definition in all_definitions:\n",
    "        for eachDef in tokenizer(definition): \n",
    "            for word in eachDef: \n",
    "                vocab.append(word)\n",
    "    vocab = set(vocab)\n",
    "    return vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n",
      "128730\n"
     ]
    }
   ],
   "source": [
    "#Separates all words in wordnet into training_set of size 20,000 and validation set of size 250 \n",
    "all_words = set(word for synset in wordnet.all_synsets() for word in synset.lemma_names())\n",
    "all_list = [word for word in all_words] #148730 total words \n",
    "\n",
    "#Randomly select 20,000 words to train on \n",
    "\n",
    "\n",
    "def sampleAndRemove(stringList, size): \n",
    "    sample = []\n",
    "    sample_indices = (random.sample(range(0,len(stringList)), size))\n",
    "    sample_dict = {}\n",
    "    remaining_words = []\n",
    "\n",
    "    # Uses dictionary of indices to remove to reduce compute time to large O(n) \n",
    "    for i in range(len(stringList)): \n",
    "        sample_dict[i] = 0\n",
    "    for index in sample_indices: \n",
    "        sample_dict[index] = 1\n",
    "\n",
    "    for index in range(len(stringList)): \n",
    "        if sample_dict[index] == 1: \n",
    "            sample.append(stringList[index])\n",
    "        else: \n",
    "            remaining_words.append(stringList[index])\n",
    "    return sample, remaining_words\n",
    "\n",
    "#See if this new return syntax actually works. if not, go back to old list of two lists format \n",
    "trainset, remaining_words  =  sampleAndRemove(all_list, 20000)\n",
    "print(len(trainset))\n",
    "print(len(remaining_words))\n",
    "validation_set, remaining_words = sampleAndRemove(remaining_words, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d i _ _ _ m i t _', '_ u _ _ p _ n']\n",
      "302564\n"
     ]
    }
   ],
   "source": [
    "def createVocab(wordset): \n",
    "    #Define primary lists based off words given \n",
    "    masked_words = [ maskWord(word) for word in wordset]\n",
    "    print(masked_words[:2])\n",
    "    definitions = [getFirstDef(word) for word in wordset]\n",
    "    tokenized_defs = [tokenizer(phrase) for phrase in definitions]\n",
    "    \n",
    "    #Build out allwords, which will serve as full vocab \n",
    "    allwords = [word for word in wordset]\n",
    "    for word in masked_words: \n",
    "        allwords.append(word)\n",
    "\n",
    "    for definition in tokenized_defs: \n",
    "        for word in definition: \n",
    "            allwords.append(word)\n",
    "\n",
    "    total_vocab = set(allwords)\n",
    "\n",
    "    print(len(total_vocab))\n",
    "\n",
    "    word2index = dict()\n",
    "    index2word = dict()\n",
    "    i = 0\n",
    "    for word in total_vocab: \n",
    "        word2index[word] = i\n",
    "        index2word[i] = word \n",
    "        i += 1\n",
    "    return total_vocab, word2index, index2word, masked_words\n",
    "\n",
    "total_vocab, word2index, index2word, all_masked = createVocab(all_list)\n",
    "vocab_size = len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Function to create dataLoader \n",
    "#Returns the dataLoader, word2index and index2word dictionaries for embedding, and vocab_size for model init \n",
    "    \n",
    "def createLoader(wordset, batch_size, all_masked):\n",
    "\n",
    "    #Build out smaller lists for specified set \n",
    "    definitions = [getFirstDef(word) for word in wordset]\n",
    "    tokenized_defs = [tokenizer(phrase) for phrase in definitions]\n",
    "\n",
    "    labels_indices = [word2index[word] for word in wordset]\n",
    "    \n",
    "    masked_indices = [word2index[all_masked[i]] for i in range(len(wordset))]\n",
    "\n",
    "    definitions_indices = []\n",
    "    for definition in tokenized_defs: \n",
    "        templist = []\n",
    "        for word in definition: \n",
    "            templist.append(word2index[word])\n",
    "        definitions_indices.append(templist)\n",
    "\n",
    "    #Introduce padding here \n",
    "    #Determine the longest definitional sequence\n",
    "    maxDef = 0 \n",
    "    for sequence in definitions_indices: \n",
    "        if len(sequence) > maxDef: \n",
    "            maxDef = len(sequence)\n",
    "\n",
    "    #Pad with 0's\n",
    "    for sequence in definitions_indices: \n",
    "        while maxDef - len(sequence) > 0: \n",
    "            sequence.append(0)\n",
    "    # Convert data to PyTorch tensors\n",
    "    labels_tensor = torch.tensor(labels_indices, dtype=torch.long)\n",
    "    masked_tensor = torch.tensor(masked_indices, dtype=torch.long)\n",
    "    definitions_tensor = torch.tensor(definitions_indices, dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(definitions_tensor, masked_tensor, labels_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "\n",
    "class predictMasked(torch.nn.Module):\n",
    "    def __init__(self, lstm_dim, masked_vocab_size, embed_dim):\n",
    "        super(predictMasked, self).__init__()\n",
    "        \n",
    "        # BERT model for processing the definition\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        \n",
    "        # An embedding layer for the masked word\n",
    "        self.masked_word_embedding = torch.nn.Embedding(masked_vocab_size, embed_dim)\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = torch.nn.LSTM(self.bert.config.hidden_size + embed_dim, lstm_dim, batch_first=True, bidirectional=True)\n",
    "\n",
    "        #Hidden layer for processing concatenation? \n",
    "        \n",
    "        # Classification layer\n",
    "        self.fc = torch.nn.Linear(2 * lstm_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, masked_word_ids):\n",
    "        # Process the definition through BERT and then lstm to adjust dimension \n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        definition_embedding = bert_output.last_hidden_state\n",
    "        \n",
    "        # Embed the masked word\n",
    "        masked_word_embedding = self.masked_word_embedding(masked_word_ids)\n",
    "        \n",
    "        # Combine the embeddings (simple concatenation here)\n",
    "        combined_embedding = torch.cat((definition_embedding, masked_word_embedding), dim=1)\n",
    "        \n",
    "        # Process the combined embedding through LSTM\n",
    "        lstm_output, _ = self.lstm(combined_embedding)\n",
    "        \n",
    "        # Use the output for prediction\n",
    "        x = self.fc(lstm_output)  \n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Instantiate dataLoader of training set - not sure if this creation is needed anymore\n",
    "batch_size = 2000\n",
    "testLoader = createLoader(trainset, batch_size, all_masked)\n",
    "validationLoader = createLoader(validation_set, 1, all_masked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "definitions = [getFirstDef(word) for word in trainset]\n",
    "masked_words = [maskWord(word) for word in trainset]\n",
    "inputs = tokenizer(definitions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "BERT_len = len(inputs['input_ids'][1])\n",
    "targets = tokenizer(trainset, padding = 'max_length', truncation = True, return_tensors = \"pt\", max_length = BERT_len)\n",
    "\n",
    "masked_vocab_size  = len(masked_words)\n",
    "masked_word2index = {}\n",
    "index2masked_word = {}\n",
    "for i in range(masked_vocab_size): \n",
    "    masked_word2index[masked_words[i]] = i\n",
    "    index2masked_word[i] = masked_words[i]\n",
    "\n",
    "masked_indices = [[masked_word2index[masked_words[i]]] for i in range(masked_vocab_size)]\n",
    "\n",
    "#Pad indexes to match BERT tokenization \n",
    "\n",
    "for masked_index in masked_indices: \n",
    "    while BERT_len - len(masked_index) > 0: \n",
    "        masked_index.append(0)\n",
    "\n",
    "masked_tensor = torch.tensor(masked_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dataset = (inputs['input_ids'], inputs['attention_mask'], masked_tensor, targets['input_ids'])\n",
    "loader = DataLoader(dataset, batch_size = batch_size, shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation loader setup \n",
    "\n",
    "val_definitions = [getFirstDef(word) for word in validation_set]\n",
    "val_masked_words = [maskWord(word) for word in validation_set]\n",
    "val_inputs = tokenizer(val_definitions, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_BERT_len = len(val_inputs['input_ids'][1])\n",
    "val_targets = tokenizer(validation_set, padding = 'max_length', truncation = True, return_tensors = \"pt\", max_length = val_BERT_len)\n",
    "\n",
    "\n",
    "#Do I need to make these together with the validation set and have one large set like when I did the LSTM myself? \n",
    "val_masked_vocab_size  = len(val_masked_words)\n",
    "val_masked_word2index = {}\n",
    "val_index2masked_word = {}\n",
    "for i in range(val_masked_vocab_size): \n",
    "    val_masked_word2index[masked_words[i]] = i\n",
    "    val_index2masked_word[i] = masked_words[i]\n",
    "\n",
    "val_masked_indices = [[val_masked_word2index[masked_words[i]]] for i in range(val_masked_vocab_size)]\n",
    "\n",
    "#Pad indexes to match BERT tokenization \n",
    "for masked_index in val_masked_indices: \n",
    "    while val_BERT_len - len(masked_index) > 0: \n",
    "        masked_index.append(0)\n",
    "\n",
    "val_masked_tensor = torch.tensor(val_masked_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = (val_inputs['input_ids'], val_inputs['attention_mask'], val_masked_tensor, val_targets['input_ids'])\n",
    "validationLoader = DataLoader(val_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Hyperparameters, model and loss function init\n",
    "embed_dim = 50\n",
    "lstm_dim = 64\n",
    "\n",
    "model = predictMasked(lstm_dim, masked_vocab_size, embed_dim)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay = 0.00005)\n",
    "\n",
    "#Training loop, make sure to update epochs to be 10 eventually and fine-tune \n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()\n",
    "    for input_ids, attention_mask, masked_index, target_id in loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, masked_index)\n",
    "        loss = criterion(outputs, target_id)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    average_loss = total_loss / batch_size\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Loss: {average_loss:.4f}')\n",
    "    \n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_defs, val_masked, val_labels in validationLoader:\n",
    "            val_outputs = model(val_masked, val_defs)\n",
    "            val_loss = criterion(val_outputs, val_labels)\n",
    "            total_val_loss += val_loss.item()\n",
    "    \n",
    "    average_val_loss = total_val_loss / len(validationLoader)\n",
    "    print(f'Validation Loss: {average_val_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize hyperparamters here; how to tune s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
