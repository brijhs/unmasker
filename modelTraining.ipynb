{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PyTorch\n",
    "import torch.nn #Parent module for pytorch for neural networks \n",
    "import torch.nn.functional as F #for the activation function \n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Hugging Face ML packages \n",
    "from transformers import pipeline, BertTokenizer, DataCollatorWithPadding, BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "unmasker = pipeline('fill-mask', model='bert-large-uncased-whole-word-masking')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking')\n",
    "from datasets import Dataset\n",
    "\n",
    "#Import wordnet synsets for vocabulary \n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "#Other math imports \n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper Functions to create dataset \n",
    "\n",
    "#Word masking function from string str with python random engine \n",
    "def maskWord(string): \n",
    "\n",
    "    #Set type to string (wordnet includes integers)\n",
    "    if type(string) != str: \n",
    "        string = str(string)\n",
    "\n",
    "    str_len = len(string)\n",
    "    if str_len == 0: \n",
    "        return \n",
    "\n",
    "    #Generate indexes based on length of word \n",
    "\n",
    "    #Don't wanna mask the entire word\n",
    "    if str_len < 3: \n",
    "        choices = random.choices(range(str_len), k = 1)\n",
    "    else: \n",
    "        choices = random.choices(range(str_len), k = random.randint(2,str_len - 1))\n",
    "\n",
    "    ##Unpack string into a list of characters\n",
    "    str_list = [*string]\n",
    "    for index in choices: \n",
    "        str_list[index] = '_'\n",
    "    newstr = ''.join(str_list)\n",
    "    return newstr\n",
    "\n",
    "#Space a masked word\n",
    "def spaceWord(word): \n",
    "    if type(word) != word: \n",
    "        word = str(word)\n",
    "\n",
    "    k = len(word)\n",
    "    wordList = [*word]\n",
    "    newList = []\n",
    "    \n",
    "    for i in range(k): \n",
    "        newList.append(wordList[i])\n",
    "        newList.append(' ')\n",
    "\n",
    "    return ''.join(newList) \n",
    "\n",
    "#Replace mask with [MASK] Special token \n",
    "def replaceMask(word): \n",
    "    strList = [*word]\n",
    "    for i in range(len(strList)): \n",
    "        if strList[i] == '_': \n",
    "            strList[i] = '[MASK]'\n",
    "    return ''.join(strList)\n",
    "\n",
    "# Final combination of functions \n",
    "def BertMaskWord(word): \n",
    "    return replaceMask(spaceWord(maskWord(word)))\n",
    "\n",
    "#Uses synsets to generate definition \n",
    "def getFirstDef(word): \n",
    "    return wordnet.synsets(word)[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleAndRemove(stringList, size): \n",
    "    sample = []\n",
    "    sample_indices = (random.sample(range(0,len(stringList)), size))\n",
    "    sample_dict = {}\n",
    "    remaining_words = []\n",
    "\n",
    "    # Uses dictionary of indices to remove to reduce compute time to large O(n) \n",
    "    for i in range(len(stringList)): \n",
    "        sample_dict[i] = 0\n",
    "    for index in sample_indices: \n",
    "        sample_dict[index] = 1\n",
    "\n",
    "    for index in range(len(stringList)): \n",
    "        if sample_dict[index] == 1: \n",
    "            sample.append(stringList[index])\n",
    "        else: \n",
    "            remaining_words.append(stringList[index])\n",
    "    return sample, remaining_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizing helper functions for mapping \n",
    "def tokenize_function(example): \n",
    "    return tokenizer(example['word'], example['word_masked'], example['definition'], padding = \"max_length\", max_length = 512)\n",
    "\n",
    "def tokenize_maskOnly(example):\n",
    "    return tokenizer(example['masked'], padding = \"max_length\", max_length = 512, truncation = True)\n",
    "    \n",
    "\n",
    "def tokenize_labels(example): \n",
    "    return tokenizer(example['word'], padding = \"max_length\", max_length = 512, truncation = True)\n",
    "\n",
    "def tokenize_combined(example): \n",
    "    return tokenizer(example['Mask&Def'], padding = \"max_length\", max_length = 512, truncation = True)\n",
    "dataCollator = DataCollatorWithPadding(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F r [MASK] n c h [MASK] h o [MASK] e y s [MASK] c [MASK] l e  French_honeysuckle\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "#Collect all words in the synset \n",
    "all_words = set(word for synset in wordnet.all_synsets() for word in synset.lemma_names())\n",
    "fullWordList = [word for word in all_words] #148730 total words \n",
    "\n",
    "#Creating training and validation set using sample and remove \n",
    "trainset, remaining_words  =  sampleAndRemove(fullWordList, 5000)\n",
    "valset, discarded_words = sampleAndRemove(remaining_words,100)\n",
    "#Discarded words are not used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63b9b1f370c4b478e43189a9c5950dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "366a6c45bf384324b215d8aa868d2a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "course_of_study\n",
      "an integrated course of academic studies [SEP] c [MASK] [MASK] r [MASK] [MASK] [MASK] [MASK] f [MASK] [MASK] t u [MASK] [MASK] \n",
      "[101, 2019, 6377, 2607, 1997, 3834, 2913, 102, 1039, 103, 103, 1054, 103, 103, 103]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating validation set for testing \n",
    "valset_maskedOnly = {'masked':[]}\n",
    "valset_labels = {'word': []}\n",
    "for i in range(len(valset)): \n",
    "    #Keep words and embeddings for testing \n",
    "    valset_labels['word'].append(valset[i])\n",
    "    valset_maskedOnly['masked'].append((\"{} [SEP] \".format(getFirstDef(valset[i])) + BertMaskWord(valset[i])))\n",
    "\n",
    "val_ls = Dataset.from_dict(valset_labels)\n",
    "val_ms = Dataset.from_dict(valset_maskedOnly)\n",
    "\n",
    "valLabels_ds = val_ls.map(tokenize_labels)\n",
    "valMasked_ds = val_ms.map(tokenize_maskOnly)\n",
    "\n",
    "#Combine labels (original word) with masked inputs \n",
    "valFinalDict = {'labels' : [], 'input_ids' : [], 'attention_mask' : []}\n",
    "valFinalDict['input_ids'] = valMasked_ds['input_ids']\n",
    "valFinalDict['attention_mask'] = valMasked_ds['attention_mask']\n",
    "valFinalDict['labels'] = valLabels_ds['input_ids']\n",
    "\n",
    "valFinal = Dataset.from_dict(valFinalDict)\n",
    "#Load into loader farther down "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G u l f _ o f _ A l a s k a \n",
      "a gulf of the Pacific Ocean between the Alaska Peninsula and the Alexander Archipelago [SEP] G u [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] A l [MASK] s [MASK] [MASK] \n"
     ]
    }
   ],
   "source": [
    "#Separated out dictionaries for flexibility during testing \n",
    "maskedOnly = {'masked': []}\n",
    "labelsSet = {'word': []}\n",
    "maskDef = {'Mask&Def': []}\n",
    "\n",
    "for i in range(len(trainset)): \n",
    "    maskedOnly['masked'].append(BertMaskWord(trainset[i]))\n",
    "    labelsSet['word'].append(spaceWord(trainset[i]))\n",
    "    maskDef['Mask&Def'].append(\"{} [SEP] \".format(getFirstDef(trainset[i])) + BertMaskWord(trainset[i]))\n",
    "\n",
    "\n",
    "#Next idea: if there's an underscore, set to 103 (mask token)\n",
    "\n",
    "maskDef = Dataset.from_dict(maskDef)\n",
    "\n",
    "maskedSet = Dataset.from_dict(maskedOnly)\n",
    "labelsSet = Dataset.from_dict(labelsSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d4ddbecae2449c8251d9b9164ae99a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc706b9a0cb24e0dbf0155d83e87172d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87856d111e1344169a8d597784713014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['word', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 5000\n",
      "})\n",
      "G u l f _ o f _ A l a s k a \n",
      "G u [MASK] f [MASK] o f [MASK] A l [MASK] [MASK] k [MASK] \n",
      "[101, 1043, 1057, 1048, 1042, 1035, 1051, 1042, 1035, 1037, 1048, 1037, 1055, 1047, 1037]\n",
      "[101, 1043, 1057, 103, 1042, 103, 1051, 1042, 103, 1037, 1048, 103, 103, 1047, 103]\n"
     ]
    }
   ],
   "source": [
    "#Creating datasets separately with each function to have different loaders at will\n",
    "labels_ds = labelsSet.map(tokenize_labels)\n",
    "masked_ds = maskedSet.map(tokenize_maskOnly)\n",
    "combined_ds = maskDef.map(tokenize_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Originally for training the model using only the spaced and masked words \n",
    "finalDict = {'labels' : [], 'input_ids' : [], 'attention_mask' : []}\n",
    "#finalDict['input_ids'] = masked_ds['input_ids']\n",
    "finalDict['input_ids'] = masked_ds['input_ids']\n",
    "finalDict['attention_mask'] = masked_ds['attention_mask']\n",
    "finalDict['labels'] = labels_ds['input_ids']\n",
    "\n",
    "final_ds = Dataset.from_dict(finalDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['labels', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 5000\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Used for training the model with blanks and definitions \n",
    "combined_final = {'labels' : [], 'input_ids' : [], 'attention_mask' : []}\n",
    "combined_final['input_ids'] = combined_ds['input_ids']\n",
    "combined_final['attention_mask'] = combined_ds['attention_mask']\n",
    "combined_final['labels'] = labels_ds['input_ids']\n",
    "\n",
    "combined_final = Dataset.from_dict(combined_final)\n",
    "\n",
    "combined_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization code for various dataLoaders for training and validation \n",
    "\n",
    "#train_tokenized = final_ds.with_format(\"torch\")\n",
    "\n",
    "valTokenized = valFinal.with_format(\"torch\")\n",
    "\n",
    "#combined_tokenized = combined_final.with_format(\"torch\")\n",
    "\n",
    "#masked_train = DataLoader(train_tokenized, batch_size = 16, collate_fn = dataCollator)\n",
    "\n",
    "#combined_train = DataLoader(combined_tokenized, batch_size = 16, collate_fn = dataCollator)\n",
    "\n",
    "val_DataLoader = DataLoader(valTokenized, batch_size = 1, collate_fn = dataCollator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100\n",
      "Epoch 1 loss: 0.20329707575301392\n",
      "Epoch 2 loss: 0.2030935439819726\n",
      "Epoch 3 loss: 0.20299759154883437\n",
      "Epoch 4 loss: 0.2029394814000724\n",
      "Epoch 5 loss: 0.20289968911070413\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'val_DataLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     36\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mval_DataLoader\u001b[49m: \n\u001b[1;32m     38\u001b[0m    optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     39\u001b[0m    input_ids \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_DataLoader' is not defined"
     ]
    }
   ],
   "source": [
    "#Training loop for attention head \n",
    "from torch import optim\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "epochs = 5\n",
    "current_loader = combined_train #For consistency and ease sake\n",
    "batchNumber = 0 \n",
    "\n",
    "for i in range(epochs): \n",
    "   total_loss = 0\n",
    "   for batch in current_loader: \n",
    "      optimizer.zero_grad()\n",
    "      input_ids = batch['input_ids']\n",
    "      attentionMask = batch['attention_mask']\n",
    "      labels = batch['labels']\n",
    "\n",
    "      outputs = model(input_ids, attention_mask = attentionMask, labels= labels)\n",
    "      #model.convert_tokens_to_string(model.convert_ids_to_tokens(outputs)) <- can't figure out how to interpret Hugging Face's .MaskedLMOutput class and view results\n",
    "\n",
    "      loss = outputs.loss\n",
    "\n",
    "      total_loss += outputs.loss\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      if batchNumber%100 == 0: \n",
    "         print(\"Batch: {}\".format(batchNumber))\n",
    "\n",
    "      batchNumber += 1   \n",
    "\n",
    "   print(\"Epoch {} loss: {}\".format(i + 1, total_loss.item()/len(current_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Push to hugging face hub\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "#notebook_login() <- hopefully I don't have to log in every time \n",
    "#Saving and pushing model\n",
    "model.save_pretrained(\"Saama-model\")\n",
    "\n",
    "model.push_to_hub(\"Saama-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload for testing \n",
    "model2 = BertForMaskedLM.from_pretrained('Brijhs/Saama-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from F import softmax #Activation function: perhaps the wrong one? \n",
    "\n",
    "#Functions for testing predictions; most results are coming out to zeroes \n",
    "def checkPrediction(prediction, discarded):\n",
    "    final = []\n",
    "    for i in range(len(prediction)): \n",
    "        if (i + 1)%64 == 0: \n",
    "            print(\"Prediction counter: {}\".format(str(i + 1)))\n",
    "        current = tokenizer.decode(prediction[i])\n",
    "        for element in current: \n",
    "            if element not in discarded: \n",
    "                final.append(element)\n",
    "    return final \n",
    "\n",
    "def interpretPrediction(outputLogits): \n",
    "    #Separated out into characters because .decode() separates by character and these are all tokens of [PAD]\n",
    "    discarded = ['[', ']', ' ', 'P', 'A', 'D']\n",
    "    sm = softmax(outputLogits, dim = -1)\n",
    "    prediction = torch.argmax(sm, dim= -1)\n",
    "    final = []\n",
    "    for i in range(len(prediction)): \n",
    "        for item in prediction[i]: \n",
    "            if item != 0: \n",
    "                final.append(item)\n",
    "                print(\"Woah\")\n",
    "\n",
    "    return checkPrediction(final, discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction loop for validation: returns all 0's \n",
    "for batch in val_DataLoader: \n",
    "    input = batch['input_ids']\n",
    "    attention_mask = batch['attention_mask']\n",
    "    output = model2(input, attention_mask = attention_mask)\n",
    "    label = batch['labels']\n",
    "\n",
    "    result = interpretPrediction(output.logits)\n",
    "    if len(result)>0: \n",
    "        print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
